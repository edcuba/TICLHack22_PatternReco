{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2fe0868",
   "metadata": {},
   "source": [
    "# Object condensation using CLUE3D\n",
    "\n",
    "Goal:\n",
    "- start with layer-clusters (x,y,z,e)\n",
    "- run edgeconv\n",
    "- collapse to tracksters\n",
    "- run edgeconv\n",
    "- fully connected\n",
    "- query edges\n",
    "- output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa0daf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric.transforms as T\n",
    "import torch_geometric.utils as geo_utils\n",
    "\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from reco.model import EdgeConvNet\n",
    "from torch_geometric.nn import EdgeConv, DynamicEdgeConv, global_mean_pool\n",
    "\n",
    "from reco.learn import train_edge_pred, test_edge_pred\n",
    "from reco.dataset import PointCloudSet\n",
    "from reco.loss import FocalLoss\n",
    "from reco.training import split_geo_train_test\n",
    "\n",
    "data_root = \"data\"\n",
    "ds_name = \"MultiParticle\"\n",
    "raw_dir = f\"/Users/ecuba/data/{ds_name}\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edc82d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive edge ratio: 0.254\n",
      "Train set: 90, Test set: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(x=[100895, 4], edge_index=[2, 32294], y=[32294], trackster_index=[100895])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = T.Compose([T.NormalizeFeatures()])\n",
    "\n",
    "ds = PointCloudSet(\n",
    "    ds_name,\n",
    "    data_root,\n",
    "    raw_dir,\n",
    "    transform=transform, # todo: z-axis transformation\n",
    "    N_FILES=1,\n",
    ")\n",
    "\n",
    "positive_edge_fr = float(sum(ds.data.y) / len(ds.data.y))\n",
    "print(f\"Positive edge ratio: {positive_edge_fr:.3f}\") \n",
    "train_dl, test_dl = split_geo_train_test(ds, batch_size=1)\n",
    "ds.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "161f1c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointCloudNet(nn.Module):\n",
    "    def __init__(self, input_dim=4, output_dim=1, aggr='add', dropout=0.2):\n",
    "        super(PointCloudNet, self).__init__()\n",
    "\n",
    "        lc_hdim1 = 32        \n",
    "        lc_hdim2 = 64\n",
    "\n",
    "        tr_hdim1 = 64\n",
    "        tr_hdim2 = 64\n",
    "\n",
    "        fc_hdim = 128\n",
    "\n",
    "        k=8\n",
    "\n",
    "        # EdgeConv on LC\n",
    "        self.lc_conv1 = DynamicEdgeConv(nn=EdgeConvNet(input_dim, lc_hdim1), aggr=aggr, k=k)\n",
    "        self.lc_conv2 = DynamicEdgeConv(nn=EdgeConvNet(lc_hdim1, lc_hdim2), aggr=aggr, k=k)\n",
    "\n",
    "        # EdgeConv on Tracksters\n",
    "        self.trackster_conv1 = DynamicEdgeConv(nn=EdgeConvNet(lc_hdim2, tr_hdim1), aggr=aggr, k=k)\n",
    "        self.trackster_conv2 = DynamicEdgeConv(nn=EdgeConvNet(tr_hdim1, tr_hdim2), aggr=aggr, k=k)\n",
    "\n",
    "        # Edge features from node embeddings for classification\n",
    "        self.edgenetwork = nn.Sequential(\n",
    "            nn.Linear(2 * tr_hdim2, fc_hdim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(fc_hdim, output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "            \n",
    "    def forward(self, X, query, tr_index):        \n",
    "\n",
    "        # tr index has gaps due to wrong reindexation on pytorch geometric with batch_size > 1\n",
    "        # last = -1\n",
    "        # idx = -1\n",
    "        # tridx2lc = {}\n",
    "        # l_tr_index = tr_index.tolist()\n",
    "        # new_idx = [0] * len(l_tr_index)\n",
    "        # for i, tr_i in enumerate(l_tr_index):\n",
    "        #     if tr_i != last:\n",
    "        #         last = tr_i\n",
    "        #         idx += 1\n",
    "        #         tridx2lc[idx] = []\n",
    "\n",
    "        #     new_idx[i] = idx\n",
    "        #     tridx2lc[idx].append(i)\n",
    "\n",
    "        # build knn edges within each trackster\n",
    "        # lc_edges = []   # knn edges witin a trackster\n",
    "\n",
    "        H = self.lc_conv1(X)\n",
    "        H = self.lc_conv2(H)\n",
    "\n",
    "        # apply per-trackster pooling of some kind (mean, add, topK, self-attention)\n",
    "        TX = global_mean_pool(H, tr_index)\n",
    "\n",
    "        H = self.trackster_conv1(TX)\n",
    "        H = self.trackster_conv2(H)\n",
    "\n",
    "        src, dst = query\n",
    "        return self.edgenetwork(torch.cat([H[src], H[dst]], dim=-1)).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "596358f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: \ttrain loss:5.88\ttrain acc: 0.676 \t test loss:9.65 \t test acc: 0.573\n",
      "Epoch 2: \ttrain loss:4.11\ttrain acc: 0.741 \t test loss:58.05 \t test acc: 0.433\n",
      "Epoch 4: \ttrain loss:3.72\ttrain acc: 0.748 \t test loss:69.88 \t test acc: 0.453\n",
      "Epoch 6: \ttrain loss:3.41\ttrain acc: 0.760 \t test loss:89.09 \t test acc: 0.434\n",
      "Epoch 8: \ttrain loss:3.20\ttrain acc: 0.765 \t test loss:75.57 \t test acc: 0.448\n",
      "Epoch 10: \ttrain loss:3.02\ttrain acc: 0.769 \t test loss:70.89 \t test acc: 0.447\n",
      "Epoch 12: \ttrain loss:2.81\ttrain acc: 0.779 \t test loss:91.76 \t test acc: 0.458\n",
      "Epoch 14: \ttrain loss:2.65\ttrain acc: 0.783 \t test loss:87.81 \t test acc: 0.454\n",
      "Epoch 16: \ttrain loss:2.50\ttrain acc: 0.788 \t test loss:98.93 \t test acc: 0.456\n",
      "Epoch 18: \ttrain loss:2.40\ttrain acc: 0.791 \t test loss:86.43 \t test acc: 0.473\n",
      "Epoch 20: \ttrain loss:2.27\ttrain acc: 0.796 \t test loss:100.10 \t test acc: 0.472\n",
      "Epoch 22: \ttrain loss:2.16\ttrain acc: 0.802 \t test loss:89.32 \t test acc: 0.460\n",
      "Epoch 24: \ttrain loss:2.05\ttrain acc: 0.807 \t test loss:123.68 \t test acc: 0.452\n",
      "Epoch 26: \ttrain loss:1.97\ttrain acc: 0.808 \t test loss:116.60 \t test acc: 0.452\n",
      "Epoch 28: \ttrain loss:1.92\ttrain acc: 0.811 \t test loss:95.35 \t test acc: 0.457\n",
      "Epoch 30: \ttrain loss:1.81\ttrain acc: 0.817 \t test loss:159.16 \t test acc: 0.461\n",
      "Epoch 32: \ttrain loss:1.75\ttrain acc: 0.816 \t test loss:118.38 \t test acc: 0.455\n",
      "Epoch 34: \ttrain loss:1.70\ttrain acc: 0.819 \t test loss:121.43 \t test acc: 0.454\n",
      "Epoch 36: \ttrain loss:1.64\ttrain acc: 0.823 \t test loss:150.72 \t test acc: 0.458\n",
      "Epoch 38: \ttrain loss:1.59\ttrain acc: 0.825 \t test loss:123.99 \t test acc: 0.474\n",
      "Epoch 40: \ttrain loss:1.52\ttrain acc: 0.829 \t test loss:125.69 \t test acc: 0.471\n",
      "Epoch 42: \ttrain loss:1.52\ttrain acc: 0.827 \t test loss:103.69 \t test acc: 0.463\n",
      "Epoch 44: \ttrain loss:1.48\ttrain acc: 0.828 \t test loss:151.18 \t test acc: 0.453\n",
      "Epoch 46: \ttrain loss:1.42\ttrain acc: 0.833 \t test loss:147.98 \t test acc: 0.457\n",
      "Epoch 48: \ttrain loss:1.39\ttrain acc: 0.832 \t test loss:151.81 \t test acc: 0.469\n"
     ]
    }
   ],
   "source": [
    "model = PointCloudNet(input_dim=ds.data.x.shape[1])\n",
    "epochs = 50\n",
    "\n",
    "# loss_func = F.binary_cross_entropy_with_logits\n",
    "# alpha - percentage of negative edges\n",
    "loss_func = FocalLoss(alpha=1.-positive_edge_fr, gamma=2)\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, epochs, eta_min=1e-3)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train_loss, train_true, train_pred = train_edge_pred(\n",
    "        model,\n",
    "        device,\n",
    "        optimizer,\n",
    "        loss_func,\n",
    "        train_dl\n",
    "    )\n",
    "    \n",
    "    train_acc = metrics.accuracy_score(train_true, (train_pred > 0.5).astype(int))\n",
    "    scheduler.step()\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        test_loss, test_true, test_pred = test_edge_pred(model, device, loss_func, test_dl)\n",
    "        test_acc = metrics.accuracy_score(test_true, (test_pred > 0.5).astype(int))\n",
    "        print(\n",
    "            f\"Epoch {epoch}:\",\n",
    "            f\"\\ttrain loss:{train_loss:.2f}\\ttrain acc: {train_acc:.3f}\",\n",
    "            f\"\\t test loss:{test_loss:.2f} \\t test acc: {test_acc:.3f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b857e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b76c464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "75d0e37245d408f3d59eb152d126431f02f862b5012558b3df6d65a37ffc466c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
